{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text from the .html file\n",
    "def get_text(html_file):\n",
    "    with open(html_file, 'r', encoding='iso-8859-1') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# function to get the bag of words from list of text using count vectorizer\n",
    "def get_bag_of_words(text_list):\n",
    "    corpus = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        #lemmatisation  \n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "        text = ' '.join(text)\n",
    "        corpus.append(text)\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(corpus)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    bag_of_words = tfidf_transformer.fit_transform(bag_of_words)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051\n",
      "1051\n",
      "1051\n",
      "1051\n",
      "(1051, 14996)\n",
      "(1051, 1725)\n"
     ]
    }
   ],
   "source": [
    "X_fulltext = []\n",
    "y_fulltext = []\n",
    "X_inlinks = []\n",
    "y_inlinks = []\n",
    "\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/fulltext/course'):\n",
    "    X_fulltext.append(get_text('course-cotrain-data/fulltext/course/'+filename))\n",
    "    y_fulltext.append('course')\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/fulltext/non-course'):\n",
    "    X_fulltext.append(get_text('course-cotrain-data/fulltext/non-course/'+filename))\n",
    "    y_fulltext.append('noncourse')\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/inlinks/course'):\n",
    "    X_inlinks.append(get_text('course-cotrain-data/inlinks/course/'+filename))\n",
    "    y_inlinks.append('course')\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/inlinks/non-course'):\n",
    "    X_inlinks.append(get_text('course-cotrain-data/inlinks/non-course/'+filename))\n",
    "    y_inlinks.append('noncourse')\n",
    "\n",
    "print(len(X_fulltext))\n",
    "print(len(y_fulltext))\n",
    "print(len(X_inlinks))\n",
    "print(len(y_inlinks))\n",
    "\n",
    "X_fulltext = get_bag_of_words(X_fulltext)\n",
    "X_inlinks = get_bag_of_words(X_inlinks)\n",
    "\n",
    "print(X_fulltext.shape)\n",
    "print(X_inlinks.shape)\n",
    "\n",
    "X_fulltext = X_fulltext.toarray()\n",
    "X_inlinks = X_inlinks.toarray()\n",
    "\n",
    "y_fulltext = [1 if x=='course' else 0 for x in y_fulltext]\n",
    "y_inlinks = [1 if x=='course' else 0 for x in y_inlinks]\n",
    "\n",
    "y_fulltext = np.array(y_fulltext)\n",
    "y_inlinks = np.array(y_inlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14996,)\n",
      "(1725,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = {'x':[], 'y':[]}\n",
    "for i in range(X_fulltext.shape[0]):\n",
    "    data['x'].append({'fulltext':X_fulltext[i],'inlinks':X_inlinks[i]})\n",
    "    data['y'].append(y_fulltext[i])\n",
    "\n",
    "print(data['x'][0]['fulltext'].shape)\n",
    "print(data['x'][0]['inlinks'].shape)\n",
    "\n",
    "print(data['y'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['x'], data['y'], test_size=0.2, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "828\n",
      "12\n",
      "828\n",
      "9\n",
      "3\n",
      "632\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# diving X_train into L and U : L has 9 non-course(0) and 3 course(1) and U has the rest\n",
    "X_train_L = []\n",
    "X_train_L1 = []\n",
    "X_train_U = []\n",
    "y_train_L = []\n",
    "y_train_L1 = []\n",
    "y_train_U = []\n",
    "\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 0 and count_0 < 9:\n",
    "        X_train_L.append(X_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_0 += 1\n",
    "    elif y_train[i] == 1 and count_1 < 3:\n",
    "        X_train_L.append(X_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_1 += 1\n",
    "    else:\n",
    "        X_train_U.append(X_train[i])\n",
    "        y_train_U.append(y_train[i])\n",
    "    \n",
    "     \n",
    "print(len(X_train_L))\n",
    "print(len(X_train_U))\n",
    "print(len(y_train_L))\n",
    "print(len(y_train_U))\n",
    "\n",
    "print(y_train_L.count(0))\n",
    "print(y_train_L.count(1))\n",
    "print(y_train_U.count(0))\n",
    "print(y_train_U.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cotraining algorithm\n",
    "def cotrain(p,n,k,u,X_train_L,X_train_U,y_train_L,y_train_U):\n",
    "    \n",
    "    for _ in range(k):\n",
    "        # Intiliazing Multinomial Naive Bayes classifiers for each view \n",
    "        clf_fulltext = MultinomialNB()\n",
    "        clf_inlinks = MultinomialNB()\n",
    "\n",
    "        fulltext_t = []\n",
    "        inlinks_t = []\n",
    "\n",
    "        for i in range(len(X_train_L)):\n",
    "            fulltext_t.append(X_train_L[i]['fulltext'])\n",
    "            inlinks_t.append(X_train_L[i]['inlinks'])\n",
    "\n",
    "\n",
    "        # Training the classifiers on the labeled data\n",
    "        clf_fulltext.fit(fulltext_t, y_train_L)\n",
    "        clf_inlinks.fit(inlinks_t, y_train_L)\n",
    "\n",
    "        # randomly select u examples from U\n",
    "        X_train_U_sample = []\n",
    "        y_train_U_sample = []\n",
    "\n",
    "        for i in range(u):\n",
    "            # randomly taking u unique examples from U\n",
    "            index = np.random.randint(len(X_train_U))\n",
    "            while index in X_train_U_sample:\n",
    "                index = np.random.randint(len(X_train_U))\n",
    "            X_train_U_sample.append(X_train_U[index])\n",
    "            y_train_U_sample.append(y_train_U[index])\n",
    "\n",
    "        X_train_U_sample_fulltext = []\n",
    "        X_train_U_sample_inlinks = []\n",
    "\n",
    "        for i in range(len(X_train_U_sample)):\n",
    "            X_train_U_sample_fulltext.append(X_train_U_sample[i]['fulltext'])\n",
    "            X_train_U_sample_inlinks.append(X_train_U_sample[i]['inlinks'])\n",
    "        \n",
    "        y_pred_fulltext = clf_fulltext.predict_proba(X_train_U_sample_fulltext)\n",
    "        y_pred_inlinks = clf_inlinks.predict_proba(X_train_U_sample_inlinks)\n",
    "\n",
    "        # adding the most confident n negative examples to L\n",
    "        for i in range(n):\n",
    "            index = np.argmax(y_pred_fulltext[:,0])\n",
    "            X_train_L.append(X_train_U_sample[index])\n",
    "            y_train_L.append(y_train_U_sample[index])\n",
    "            y_pred_fulltext = np.delete(y_pred_fulltext,index,0)\n",
    "            X_train_U_sample = np.delete(X_train_U_sample,index,0)\n",
    "            y_train_U_sample = np.delete(y_train_U_sample,index,0)\n",
    "            y_pred_inlinks = np.delete(y_pred_inlinks,index,0)\n",
    "\n",
    "        \n",
    "        # adding the most confident p positive examples to L\n",
    "        for i in range(p):\n",
    "            index = np.argmax(y_pred_fulltext[:,1])\n",
    "            X_train_L.append(X_train_U_sample[index])\n",
    "            y_train_L.append(y_train_U_sample[index])\n",
    "            y_pred_fulltext = np.delete(y_pred_fulltext,index,0)\n",
    "            X_train_U_sample = np.delete(X_train_U_sample,index,0)\n",
    "            y_train_U_sample = np.delete(y_train_U_sample,index,0)\n",
    "            y_pred_inlinks = np.delete(y_pred_inlinks,index,0)\n",
    "\n",
    "        # adding the most confident n negative examples to L\n",
    "        for i in range(n):\n",
    "            index = np.argmax(y_pred_inlinks[:,0])\n",
    "            X_train_L.append(X_train_U_sample[index])\n",
    "            y_train_L.append(y_train_U_sample[index])            \n",
    "            y_pred_inlinks = np.delete(y_pred_inlinks,index,0)\n",
    "            X_train_U_sample = np.delete(X_train_U_sample,index,0)\n",
    "            y_train_U_sample = np.delete(y_train_U_sample,index,0)\n",
    "\n",
    "        # adding the most confident p positive examples to L\n",
    "        for i in range(p):\n",
    "            index = np.argmax(y_pred_inlinks[:,1])\n",
    "            X_train_L.append(X_train_U_sample[index])\n",
    "            y_train_L.append(y_train_U_sample[index])\n",
    "            y_pred_inlinks = np.delete(y_pred_inlinks,index,0)\n",
    "            X_train_U_sample = np.delete(X_train_U_sample,index,0)\n",
    "            y_train_U_sample = np.delete(y_train_U_sample,index,0)\n",
    "            \n",
    "    return clf_fulltext, clf_inlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the trained classifiers\n",
    "clf_fulltext, clf_inlinks = cotrain(1,3,30,75,X_train_L,X_train_U,y_train_L,y_train_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.9241706161137441\n"
     ]
    }
   ],
   "source": [
    "# Testing the classifiers\n",
    "X_test_fulltext = []\n",
    "X_test_inlinks = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test_fulltext.append(X_test[i]['fulltext'])\n",
    "    X_test_inlinks.append(X_test[i]['inlinks'])\n",
    "\n",
    "y_pred_fulltext = clf_fulltext.predict_proba(X_test_fulltext)\n",
    "y_pred_inlinks = clf_inlinks.predict_proba(X_test_inlinks)\n",
    "\n",
    "# calculating the accuracy by taking the maximum of the probabilities of the two classifiers\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(y_pred_fulltext)):\n",
    "    # maximum probability from fulltext classifier\n",
    "    max_prob1 = max(y_pred_fulltext[i])\n",
    "    index1 = np.argmax(y_pred_fulltext[i])\n",
    "    # maximum probability from inlinks classifier\n",
    "    max_prob2 = max(y_pred_inlinks[i])\n",
    "    index2 = np.argmax(y_pred_inlinks[i])\n",
    "    # taking the maximum of the two probabilities\n",
    "    if max_prob1 > max_prob2:\n",
    "        y_pred.append(index1)\n",
    "    else:\n",
    "        y_pred.append(index2)\n",
    "\n",
    "# calculating the accuracy\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(count/len(y_pred_fulltext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.8530805687203792\n"
     ]
    }
   ],
   "source": [
    "# supervised training\n",
    "# Intiliazing Multinomial Naive Bayes classifiers for each view\n",
    "clf_fulltext_sup = MultinomialNB()\n",
    "clf_inlinks_sup = MultinomialNB()\n",
    "\n",
    "fulltext_t = []\n",
    "inlinks_t = []\n",
    "\n",
    "for i in range(len(X_train_L1)):\n",
    "    fulltext_t.append(X_train_L1[i]['fulltext'])\n",
    "    inlinks_t.append(X_train_L1[i]['inlinks'])\n",
    "\n",
    "# Training the classifiers on the labeled data\n",
    "clf_fulltext_sup.fit(fulltext_t, y_train_L1)\n",
    "clf_inlinks_sup.fit(inlinks_t, y_train_L1)\n",
    "\n",
    "X_test_fulltext = []\n",
    "X_test_inlinks = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test_fulltext.append(X_test[i]['fulltext'])\n",
    "    X_test_inlinks.append(X_test[i]['inlinks'])\n",
    "\n",
    "y_pred_fulltext = clf_fulltext_sup.predict_proba(X_test_fulltext)\n",
    "y_pred_inlinks = clf_inlinks_sup.predict_proba(X_test_inlinks)\n",
    "\n",
    "y_pred= []\n",
    "# Calculating the accuracy\n",
    "count = 0\n",
    "for i in range(len(y_pred_fulltext)):\n",
    "    # maximum probability from fulltext classifier\n",
    "    max_prob1 = max(y_pred_fulltext[i])\n",
    "    index1 = np.argmax(y_pred_fulltext[i])\n",
    "    # maximum probability from inlinks classifier\n",
    "    max_prob2 = max(y_pred_inlinks[i])\n",
    "    index2 = np.argmax(y_pred_inlinks[i])\n",
    "    # taking the maximum of the two probabilities\n",
    "    if max_prob1 > max_prob2:\n",
    "        y_pred.append(index1)\n",
    "    else:\n",
    "        y_pred.append(index2)\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(count/len(y_pred_fulltext))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
