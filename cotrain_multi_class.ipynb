{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text from the .html file\n",
    "def get_text(html_file):\n",
    "    with open(html_file, 'r', encoding='iso-8859-1') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# function to get the bag of words from list of text using count vectorizer\n",
    "def get_bag_of_words(text_list):\n",
    "    corpus = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        #lemmatisation  \n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "        text = ' '.join(text)\n",
    "        corpus.append(text)\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(corpus)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    bag_of_words = tfidf_transformer.fit_transform(bag_of_words)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fulltext = []\n",
    "Y_fulltext = []\n",
    "X_inlink = []\n",
    "Y_inlink = []\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/fulltext/non-course'):\n",
    "    for folder in os.listdir('webkb'):\n",
    "        if folder == 'department':\n",
    "            continue\n",
    "        for folder1 in os.listdir('webkb/' + folder):\n",
    "            if filename in os.listdir('webkb/' + folder + '/' + folder1):\n",
    "                Y_fulltext.append(folder)\n",
    "                X_fulltext.append(get_text('webkb/' + folder + '/' + folder1 + '/' + filename))\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/inlinks/non-course'):\n",
    "    for folder in os.listdir('webkb'):\n",
    "        if folder == 'department':\n",
    "            continue\n",
    "        for folder1 in os.listdir('webkb/' + folder):\n",
    "            if filename in os.listdir('webkb/' + folder + '/' + folder1):\n",
    "                Y_inlink.append(folder)\n",
    "                X_inlink.append(get_text('course-cotrain-data/inlinks/non-course/'+filename))\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/fulltext/course'):\n",
    "    X_fulltext.append(get_text('course-cotrain-data/fulltext/course/'+filename))\n",
    "    Y_fulltext.append('course')\n",
    "\n",
    "for filename in os.listdir('course-cotrain-data/inlinks/course'):\n",
    "    X_inlink.append(get_text('course-cotrain-data/inlinks/course/'+filename))\n",
    "    Y_inlink.append('course')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fulltext = get_bag_of_words(X_fulltext)\n",
    "X_inlink = get_bag_of_words(X_inlink)\n",
    "\n",
    "X_fulltext = X_fulltext.toarray()\n",
    "X_inlink = X_inlink.toarray()\n",
    "\n",
    "# label encoding\n",
    "labelencoder = LabelEncoder()\n",
    "Y_fulltext = labelencoder.fit_transform(Y_fulltext)\n",
    "Y_inlink = labelencoder.fit_transform(Y_inlink)\n",
    "\n",
    "Y_fulltext = np.array(Y_fulltext)\n",
    "Y_inlink = np.array(Y_inlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14995,)\n",
      "(1682,)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "data = {'x':[], 'y':[]}\n",
    "for i in range(len(X_fulltext)):\n",
    "    data['x'].append({'fulltext':X_fulltext[i], 'inlinks':X_inlink[i]})\n",
    "    data['y'].append(Y_fulltext[i])\n",
    "\n",
    "print(data['x'][0]['fulltext'].shape)\n",
    "print(data['x'][0]['inlinks'].shape)\n",
    "print(data['y'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['x'], data['y'], test_size=0.2, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diving X_train into L and U : L has 5 0's, 3 1's, 2 2's, 1 3's, 12 4's\n",
    "X_train_L = []\n",
    "X_train_U = []\n",
    "y_train_L = []\n",
    "y_train_U = []\n",
    "X_train_L1 = []\n",
    "y_train_L1 = []\n",
    "\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 0 and count_0 < 5:\n",
    "        X_train_L.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_0 += 1\n",
    "    elif y_train[i] == 1 and count_1 < 3:\n",
    "        X_train_L.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_1 += 1\n",
    "    elif y_train[i] == 2 and count_2 < 2:\n",
    "        X_train_L.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_2 += 1\n",
    "    elif y_train[i] == 3 and count_3 < 1:\n",
    "        X_train_L.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_3 += 1\n",
    "    elif y_train[i] == 4 and count_4 < 12:\n",
    "        X_train_L.append(X_train[i])\n",
    "        y_train_L.append(y_train[i])\n",
    "        X_train_L1.append(X_train[i])\n",
    "        y_train_L1.append(y_train[i])\n",
    "        count_4 += 1\n",
    "    else:\n",
    "        X_train_U.append(X_train[i])\n",
    "        y_train_U.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cotrain(a,b,c,d,e,k,u,X_train_L,X_train_U,y_train_L,y_train_U):\n",
    "    X_train_L__fulltext = X_train_L\n",
    "    X_train_L__inlinks = X_train_L\n",
    "    y_train_L__fulltext = y_train_L\n",
    "    y_train_L__inlinks = y_train_L\n",
    "    for _ in range(k):\n",
    "        clf_fulltext = MultinomialNB()\n",
    "        clf_inlinks = MultinomialNB()\n",
    "        fulltext_t = []\n",
    "        inlinks_t = []\n",
    "\n",
    "        for i in range(len(X_train_L__fulltext)):\n",
    "            fulltext_t.append(X_train_L__fulltext[i]['fulltext'])\n",
    "        \n",
    "        for i in range(len(X_train_L__inlinks)):\n",
    "            inlinks_t.append(X_train_L__inlinks[i]['inlinks'])\n",
    "        \n",
    "        clf_fulltext.fit(fulltext_t,y_train_L__fulltext)\n",
    "        clf_inlinks.fit(inlinks_t,y_train_L__inlinks)\n",
    "\n",
    "        # randomly select u examples from U\n",
    "        X_train_U_sample_f = []\n",
    "        y_train_U_sample_f = []\n",
    "        X_train_U_sample_i = []\n",
    "        y_train_U_sample_i = []\n",
    "\n",
    "        for i in range(u):\n",
    "            # randomly taking u unique examples from U\n",
    "            index = np.random.randint(len(X_train_U))\n",
    "            while index in X_train_U_sample_f:\n",
    "                index = np.random.randint(len(X_train_U))\n",
    "            X_train_U_sample_f.append(X_train_U[index])\n",
    "            y_train_U_sample_f.append(y_train_U[index])\n",
    "            X_train_U_sample_i.append(X_train_U[index])\n",
    "            y_train_U_sample_i.append(y_train_U[index])\n",
    "        \n",
    "        X_train_U_sample_fulltext = []\n",
    "        X_train_U_sample_inlinks = []\n",
    "\n",
    "        for i in range(len(X_train_U_sample_f)):\n",
    "            X_train_U_sample_fulltext.append(X_train_U_sample_f[i]['fulltext'])\n",
    "            X_train_U_sample_inlinks.append(X_train_U_sample_f[i]['inlinks'])\n",
    "        \n",
    "        y_pred_fulltext = clf_fulltext.predict_proba(X_train_U_sample_fulltext)\n",
    "        y_pred_inlinks = clf_inlinks.predict_proba(X_train_U_sample_inlinks)\n",
    "\n",
    "        # adding the most confident a 0's, b 1's, c 2's, d 3's, e 4's to X_train_L__fulltext and X_train_L__inlinks\n",
    "\n",
    "        arr = [a,b,c,d,e]\n",
    "        for i in range(len(arr)):\n",
    "            for j in range(arr[i]):\n",
    "                index = np.argmax(y_pred_fulltext[:,i])\n",
    "                X_train_L__inlinks.append(X_train_U_sample_f[index])\n",
    "                y_train_L__inlinks.append(y_train_U_sample_f[index])\n",
    "                y_pred_fulltext = np.delete(y_pred_fulltext,index,axis=0)\n",
    "                X_train_U_sample_f = np.delete(X_train_U_sample_f,index,axis=0)\n",
    "                y_train_U_sample_f = np.delete(y_train_U_sample_f,index,axis=0)\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            for j in range(arr[i]):\n",
    "                index = np.argmax(y_pred_inlinks[:,i])\n",
    "                X_train_L__fulltext.append(X_train_U_sample_i[index])\n",
    "                y_train_L__fulltext.append(y_train_U_sample_i[index])\n",
    "                y_pred_inlinks = np.delete(y_pred_inlinks,index,axis=0)\n",
    "                X_train_U_sample_i = np.delete(X_train_U_sample_i,index,axis=0)\n",
    "                y_train_U_sample_i = np.delete(y_train_U_sample_i,index,axis=0)\n",
    "\n",
    "    return clf_fulltext,clf_inlinks        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the trained classifiers\n",
    "clf_fulltext,clf_inlinks = cotrain(5,3,2,1,12,10,150,X_train_L,X_train_U,y_train_L,y_train_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6619047619047619\n"
     ]
    }
   ],
   "source": [
    "# Testing the classifiers\n",
    "X_test_fulltext = []\n",
    "X_test_inlinks = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test_fulltext.append(X_test[i]['fulltext'])\n",
    "    X_test_inlinks.append(X_test[i]['inlinks'])\n",
    "\n",
    "y_pred_fulltext = clf_fulltext.predict_proba(X_test_fulltext)\n",
    "y_pred_inlinks = clf_inlinks.predict_proba(X_test_inlinks)\n",
    "\n",
    "# calculating the accuracy by taking the maximum of the probabilities of the two classifiers\n",
    "y_pred = []\n",
    "for i in range(len(y_pred_fulltext)):\n",
    "    # maximum probability from fulltext classifier\n",
    "    max_prob1 = np.max(y_pred_fulltext[i])\n",
    "    index1 = np.argmax(y_pred_fulltext[i])\n",
    "    # maximum probability from inlinks classifier\n",
    "    max_prob2 = np.max(y_pred_inlinks[i])\n",
    "    index2 = np.argmax(y_pred_inlinks[i])\n",
    "    if max_prob1 > max_prob2:\n",
    "        y_pred.append(index1)\n",
    "    else:\n",
    "        y_pred.append(index2)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# calculating the accuracy\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print('Accuracy:',count/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49047619047619045\n"
     ]
    }
   ],
   "source": [
    "# supervised training\n",
    "# Intiliazing Multinomial Naive Bayes classifiers for each view\n",
    "clf_fulltext_sup = MultinomialNB()\n",
    "clf_inlinks_sup = MultinomialNB()\n",
    "\n",
    "fulltext_t = []\n",
    "inlinks_t = []\n",
    "\n",
    "for i in range(len(X_train_L1)):\n",
    "    fulltext_t.append(X_train_L1[i]['fulltext'])\n",
    "    inlinks_t.append(X_train_L1[i]['inlinks'])\n",
    "\n",
    "# Training the classifiers on the labeled data\n",
    "clf_fulltext_sup.fit(fulltext_t, y_train_L1)\n",
    "clf_inlinks_sup.fit(inlinks_t, y_train_L1)\n",
    "\n",
    "y_pred_fulltext = clf_fulltext_sup.predict_proba(X_test_fulltext)\n",
    "y_pred_inlinks = clf_inlinks_sup.predict_proba(X_test_inlinks)\n",
    "\n",
    "# calculating the accuracy by taking the maximum of the probabilities of the two classifiers\n",
    "y_pred = []\n",
    "for i in range(len(y_pred_fulltext)):\n",
    "    # maximum probability from fulltext classifier\n",
    "    max_prob1 = np.max(y_pred_fulltext[i])\n",
    "    index1 = np.argmax(y_pred_fulltext[i])\n",
    "    # maximum probability from inlinks classifier\n",
    "    max_prob2 = np.max(y_pred_inlinks[i])\n",
    "    index2 = np.argmax(y_pred_inlinks[i])\n",
    "    if max_prob1 > max_prob2:\n",
    "        y_pred.append(index1)\n",
    "    else:\n",
    "        y_pred.append(index2)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# calculating the accuracy\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print('Accuracy:',count/len(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
